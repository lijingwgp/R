---
title: "Clustering on Mixed Data Types"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clustering allows us to better understand how a sample might be comprised of distinct subgroups given a set of variables. While many tutorials typically review a simple application using continuous variables, clustering data of mixed types (continuous, ordianl and nonminal) is often of interest. 

The following is an overview of one approach to clustering data of mixed types using Gower distance, partitioning around medoids, and silhouette width.

In total, there are three related decisions that need to be taken for this approach:

  1. Calculating distance
  2. Choosing a clustering algorithm
  3. Selecting the number of clusters
  
For illustration, the publicly available "College" dataset found in the ISLR package will be used. The following is a list of continuous and categorical variables.

  1. Continuous
    a. Acceptance rate
    b. Out of school tuition
    c. Number of new students enrolled
    
  2. Categorical
    a. Whether a college is public/private
    b. Whether a college is elite, defined as having more than 50% of new students who graduated in the top 10% of their high school class

We will be using the following packages:

```{r}
set.seed(1680)
library(dplyr)
library(ISLR)
library(cluster)
library(Rtsne)
library(ggplot2)
```

Before clustering can begin, some data cleaning must be done:

  1. Acceptance rate is created by dividing the number of acceptances by the number of applications
  2. isElite is created by labeling colleges with more than 50% of their new students who were in the top 10% of their high school class as elite
  
```{r}
college_clean <- College %>%
  mutate(name = row.names(.),
         accept_rate = Accept/Apps,
         isElite = cut(Top10perc,
                       breaks = c(0, 50, 100),
                       labels = c("Not Elite", "Elite"),
                       include.lowest = TRUE)) %>%
  mutate(isElite = factor(isElite)) %>%
  select(name, accept_rate, Outstate, Enroll,
         Grad.Rate, Private, isElite)
glimpse(college_clean)
```

# Calculating Distance
In order for a yet-to-be-chosen algorithm to group observations together, we first need to define some notion of (dis)similarity between observation. A popular choice for clustering is Euclidean distance. However Euclidean distance is only valid for continuous variables and thus is not applicable here.

In order or a clustering algorithm to yield sensible results, we have to use something that can handle mixed data types. In this case, we will use Gower distance.

## Gower distance
The concept of Gower distance is actually quite simple, For each variable type, a particular distance metric that works well for that type is used and scaled to fall between 0 and 1. 

Then a linear combination using user-specified weights is calculated to create the final distance matrix. The metrics used for each dat type are described below:

  1. quantitative (interval): range-normalized Manhattan distance
  2. ordinal: variable is first ranked, then Manhattan distance is used with a special adjustment for ties
  3. nominal: variables of k categories are first converted into k binary columns and then the Dice coefficient is used
  
Below, we see that Gower distance can be calculated in one line using the daisy function. Note that due to positive skew in the Enroll variable, a log transformation is conducted internally via the type argument.

```{r}
# remove college name before clustering
gower_dist <- daisy(college_clean[,-1],
                    metric = 'gower',
                    type = list(logratio=3))
# check attributes to ensure the correct methods are being used
# (I = interval, N = nomial)
summary(gower_dist)
```

As a sanity check, we can print out the most similar and dissimilar pair in the data to see if it makes sense.

```{r}
gower_mat <- as.matrix(gower_dist)
# output most similar pair
college_clean[
  which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]),
        arr.ind = TRUE)[1, ], ]
# output most dissimilar pair
college_clean[
  which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]),
        arr.ind = TRUE)[1, ], ]
```

# Choosing a clustering algorithm
Now that the distance matrix has been calculated, it is time to select an algorithm for clustering. While manhy algorithms that can handle a custom distance matrix, partitioning around medoids (PAM) will be used here.

PAM is an iterative clustering procedure with the following steps:

  1. Choose k random entities to become the medoids
  2. Assign every entity to its closest medoid 
  3. For each cluster, identify the observation that would yield the lowest average distance if it were to be re-assigned as the medoid. If so, make this observation the new medoid
  4. If at least one medoid has changed, return to step 2. Otherwise, end the algorithm

The PAM is very similar to k-means algorithm. In fact, both approaches are identical, except k-means has cluster centers defined by Euclidean distance, while cluster centers for PAM are restricted to be the observations themselves.

# Selecting the number of clusters
A variety of metrics exist to help choose the number of clusters to be extracted in a cluster anlaysis. We will use silhouette width, an internal validation metric which is an aggregated measure of how similar an observation is to its own cluster compared its closest neighboring cluster.

The metric can range from -1 to 1, where higher values are better. After calculating silhouette width for clusters ranging from 2 to 10 for the PAM algorithm, we see that 3 clusters yields the highest value.

```{r}
# calculate silhouette width for many k using PAM
sil_width <- c(NA)
for (i in 2:10) {
  pam_fit <- pam(gower_dist,
                 diss = TRUE,
                 k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}
# plot sihouette width (higher is better)
plot(1:10, sil_width,
     xlab = "Number of clusters",
     ylab = "Silhouette Width")
lines(1:10, sil_width)
```

# Cluster Interpretation
## Via Descriptive Statistics
After running the algorithm and selecting three clusters, we can interpret the clusters by running summary on each cluster.

```{r}
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)
pam_results <- college_clean %>%
  dplyr::select(-name) %>%
  mutate(cluster = pam_fit$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))
pam_results$the_summary
```

Based on these results:

  1. It seems as though Cluster 1 is mainly Private/Not Elite with medium levels of out of state tuition and smaller levels of enrollment. 
  2. Cluster 2, on the other hand, is mainly Private/Elite with lower levels of acceptance rates, high levels of out of state tuition, and high graduation rates.
  3. Cluster 3 is mainly Public/Not Elite with the lowest levels of tuition, largest levels of enrollment, and lowest graduation rate.

Another benefit of the PAM algorithm with respect to interpretation is that the medoids serve as exemplars of each cluster. From this, we see that Saint Francis University is the medoid of the Private/Not Elite cluster, Barnard College is the medoid for the Private/Elite cluster, and Grand Valley State University is the medoid for the Public/Not Elite cluster.

## Via Visualization
One way to visualize many variables in a lower dimensional space is with t-distributed stochastic neighborhood embedding. This method is a dimension reduction technique that tries to preserve local structure so as to make clusters visible in a 2D or 3D visualization. 

While it typically utilizes Euclidean distance, it has the ability to handle a custom distance metric like the one we created above. 

```{r}
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering),
         name = college_clean$name)
ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```

One curious thing to note is that there is a small group that is split between the Private/Elite cluster and the Public/Not Elite cluster.

By investigating further, it looks like this group is made up of the larger, more competitive public schools, like the University of Virginia or the University of California at Berkeley. While not large enough to warrant an additional cluster.

According to silhouette width, these 13 schools certainly have characteristics distinct from the other three clusters.

```{r}
tsne_data %>%
  filter(X > -5 & X < 0,
         Y > 20 & Y < 25) %>%
  left_join(college_clean, by = "name") %>%
  collect %>%
  .[["name"]]
```